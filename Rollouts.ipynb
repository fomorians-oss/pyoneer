{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import math\n",
    "import gym\n",
    "import redis\n",
    "import time\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pyoneer as pynr\n",
    "import pyoneer.rl as pyrl\n",
    "\n",
    "# Seed the environment.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_env_model(batch_size=None):\n",
    "    # Create the gym env, wrapped in a vectorized manner.\n",
    "    env_spec = 'CartPole-v1'\n",
    "\n",
    "    if batch_size is None:\n",
    "        gym_env = gym.make(env_spec)\n",
    "    else:\n",
    "        gym_env = pyrl.wrappers.Batch(lambda: gym.make(env_spec), batch_size)\n",
    "\n",
    "    # Wrap it in a Model.\n",
    "    env_model = pyrl.rollouts.Env(gym_env)\n",
    "    return env_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.00562286 -0.01017127 -0.04766121  0.04716188]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "env_model = create_env_model(1)\n",
    "env_outputs = env_model.reset()\n",
    "print(env_outputs.next_state)\n",
    "env_model.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "- On-policy PPO\n",
    "- Model-based, on-policy PPO\n",
    "- On/Off-policy IMPALA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentPolicyOutput = collections.namedtuple(\n",
    "    'AgentPolicyOutput', ['action', 'log_prob'])\n",
    "AgentValueOutput = collections.namedtuple(\n",
    "    'AgentValueOutput', ['value'])\n",
    "AgentPolicyValueOutput = collections.namedtuple(\n",
    "    'AgentPolicyValueOutput', ['log_prob', 'entropy', 'value'])\n",
    "\n",
    "\n",
    "class CartPoleAgent(tf.Module):\n",
    "\n",
    "    def __init__(self, action_spec):\n",
    "        super(CartPoleAgent, self).__init__(name='CartPoleAgent')\n",
    "        self._hidden = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        self._logits = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(2)])\n",
    "        self._value = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(1)])\n",
    "        self._policy = tfp.distributions.Categorical\n",
    "\n",
    "        self.action_spec = action_spec\n",
    "        self.log_prob_spec = tf.nest.map_structure(\n",
    "            lambda spec: tf.TensorSpec([], tf.dtypes.float32),\n",
    "            self.action_spec)\n",
    "        self.output_specs = AgentPolicyOutput(\n",
    "            action=self.action_spec,\n",
    "            log_prob=self.log_prob_spec)\n",
    "        self.output_shapes = tf.nest.map_structure(\n",
    "            lambda spec: spec.shape, self.output_specs)\n",
    "        self.output_dtypes = tf.nest.map_structure(\n",
    "            lambda spec: spec.dtype, self.output_specs)\n",
    "\n",
    "    @tf.function\n",
    "    def _scale_state(self, state):\n",
    "        state = (state / [[2.4, 10., 1., 10.]])\n",
    "        state = tf.concat(\n",
    "            [state, tf.stack([tf.math.cos(state[..., 2] / math.pi),\n",
    "                              tf.math.sin(state[..., 2] / math.pi)],\n",
    "                             axis=-1)],\n",
    "            axis=-1)\n",
    "        return tf.clip_by_value(state, -1., 1.)\n",
    "\n",
    "    @tf.function\n",
    "    def initialize(self, env_outputs, agent_outputs):\n",
    "        state = self._scale_state(env_outputs.state)\n",
    "        hidden = self._hidden(state)\n",
    "        _ = self._value(hidden)\n",
    "        _ = self._logits(hidden)\n",
    "    \n",
    "    @tf.function\n",
    "    def value(self, env_outputs, agent_outputs):\n",
    "        state = self._scale_state(env_outputs.state)\n",
    "        hidden = self._hidden(state)\n",
    "        value = tf.squeeze(self._value(hidden), axis=-1)\n",
    "        return AgentValueOutput(value=value)\n",
    "\n",
    "    @tf.function\n",
    "    def policy_value(self, env_outputs, agent_outputs):\n",
    "        state = self._scale_state(env_outputs.state)\n",
    "        hidden = self._hidden(state)\n",
    "        logits = self._logits(hidden)\n",
    "        policy = self._policy(logits=logits)\n",
    "        entropy = policy.entropy()\n",
    "        log_prob = policy.log_prob(agent_outputs.action)\n",
    "        value = tf.squeeze(self._value(hidden), axis=-1)\n",
    "        return AgentPolicyValueOutput(log_prob=log_prob,\n",
    "                                      entropy=entropy,\n",
    "                                      value=value)\n",
    "\n",
    "    @tf.function\n",
    "    def policy_value_with_nexts(self, env_outputs, agent_outputs):\n",
    "        # Add bootstrap state\n",
    "        def bootstrap_state(s_t, s_tp1):\n",
    "            return tf.concat([s_t, s_tp1[:, -1:]], axis=1)\n",
    "\n",
    "        bootstrapped_state = tf.nest.map_structure(\n",
    "            bootstrap_state, env_outputs.state, env_outputs.next_state)\n",
    "\n",
    "        state = self._scale_state(bootstrapped_state)\n",
    "        hidden = self._hidden(state)\n",
    "        value = tf.squeeze(self._value(hidden), axis=-1)\n",
    "\n",
    "        logits = self._logits(hidden[:, :-1])\n",
    "        policy = self._policy(logits=logits)\n",
    "        log_prob = policy.log_prob(agent_outputs.action)\n",
    "        entropy = policy.entropy()\n",
    "\n",
    "        outputs = AgentPolicyValueOutput(log_prob=log_prob,\n",
    "                                         entropy=entropy,\n",
    "                                         value=value[:, :-1])\n",
    "        bootstrap = AgentValueOutput(value=value[:, -1])\n",
    "        return outputs, bootstrap\n",
    "\n",
    "    @tf.function\n",
    "    def reset(self, env_outputs, explore=True):\n",
    "        initial_action = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([env_outputs.state.shape[0]]), \n",
    "            self.action_spec, \n",
    "            tf.zeros)\n",
    "        initial_log_prob = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([env_outputs.state.shape[0]]), \n",
    "            self.log_prob_spec, \n",
    "            tf.zeros)\n",
    "        return AgentPolicyOutput(\n",
    "            action=initial_action,\n",
    "            log_prob=initial_log_prob)\n",
    "\n",
    "    @tf.function\n",
    "    def step(self, env_outputs, agent_outputs, time_step, explore=True):\n",
    "        state = env_outputs.next_state\n",
    "        state = self._scale_state(state)\n",
    "        hidden = self._hidden(state)\n",
    "        logits = self._logits(hidden)\n",
    "        policy = self._policy(logits=logits)\n",
    "\n",
    "        if explore:\n",
    "            action = policy.sample()\n",
    "        else:\n",
    "            action = policy.mode()\n",
    "\n",
    "        action = tf.nest.map_structure(\n",
    "            lambda t, s: tf.cast(t, s.dtype), \n",
    "            action, self.action_spec)\n",
    "        log_prob = policy.log_prob(action)\n",
    "        return AgentPolicyOutput(action=action,\n",
    "                                 log_prob=log_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy(object):\n",
    "\n",
    "    def __init__(self, agent, explore):\n",
    "        self.agent = agent\n",
    "        self.explore = explore\n",
    "\n",
    "    @tf.function\n",
    "    def reset(self, *args, **kwargs):\n",
    "        return self.agent.reset(*args, explore=self.explore, **kwargs)\n",
    "\n",
    "    @tf.function\n",
    "    def step(self, *args, **kwargs):\n",
    "        return self.agent.step(*args, explore=self.explore, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParameters = collections.namedtuple(\n",
    "    'HyperParameters', \n",
    "    ['iterations',\n",
    "     'epochs',\n",
    "     'discounts',\n",
    "     'lambdas',\n",
    "     'epsilon',\n",
    "     'value_scale',\n",
    "     'entropy_scale',\n",
    "     'eval_every',\n",
    "     'learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-Policy PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 14:01:32.349082 123145399345152 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0903 14:01:37.568933 123145398272000 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 14:01:37.571855 123145398272000 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 14:01:37.575312 123145398808576 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 14:01:37.579843 123145398272000 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.71875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0903 14:01:43.949117 140736428594112 deprecation.py:323] From /Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.28125\n",
      "9.5625\n",
      "9.09375\n",
      "9.25\n",
      "9.125\n",
      "9.4375\n",
      "9.21875\n",
      "9.40625\n",
      "10.40625\n",
      "13.71875\n",
      "88.84375\n",
      "9.875\n",
      "9.3125\n",
      "9.1875\n",
      "9.78125\n",
      "10.6875\n",
      "10.21875\n",
      "13.3125\n",
      "20.40625\n",
      "28.78125\n",
      "42.5625\n",
      "65.4375\n",
      "181.90625\n",
      "450.96875\n",
      "433.84375\n",
      "422.625\n",
      "447.03125\n",
      "56\n",
      "18.03125\n",
      "25.59375\n",
      "87.15625\n",
      "184.5625\n",
      "150.15625\n",
      "359.78125\n",
      "87.03125\n",
      "99.4375\n",
      "491.1875\n",
      "148.3125\n",
      "85.5625\n",
      "377.21875\n",
      "104.3125\n",
      "121.4375\n",
      "449.21875\n",
      "80.78125\n",
      "61.03125\n",
      "372.5\n",
      "62.3125\n",
      "52.0625\n",
      "376.5625\n"
     ]
    }
   ],
   "source": [
    "explore_size = 32\n",
    "exploit_size = 16\n",
    "max_steps = 500\n",
    "\n",
    "explore_env_model = create_env_model(explore_size)\n",
    "exploit_env_model = create_env_model(explore_size)\n",
    "\n",
    "agent_model = CartPoleAgent(explore_env_model.action_spec)\n",
    "\n",
    "explore_strategy = Strategy(agent_model, True)\n",
    "exploit_strategy = Strategy(agent_model, False)\n",
    "\n",
    "explore_rollout = pyrl.rollouts.Rollout(explore_env_model, explore_strategy, max_steps)\n",
    "exploit_rollout = pyrl.rollouts.Rollout(exploit_env_model, exploit_strategy, max_steps)\n",
    "\n",
    "hparams = HyperParameters(\n",
    "    iterations=50,\n",
    "    epochs=5,\n",
    "    discounts=.99,\n",
    "    lambdas=.975,\n",
    "    epsilon=.2,\n",
    "    value_scale=.5,\n",
    "    entropy_scale=.05,\n",
    "    eval_every=1,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(hparams.learning_rate)\n",
    "discounted_returns = tf.function(pyrl.targets.discounted_returns)\n",
    "generalized_advantage_estimate = tf.function(pyrl.targets.generalized_advantage_estimate)\n",
    "\n",
    "mock_env_outputs = pynr.debugging.mock_spec(\n",
    "    tf.TensorShape([1, max_steps]), explore_env_model.output_specs)\n",
    "mock_agent_outputs = pynr.debugging.mock_spec(\n",
    "    tf.TensorShape([1, max_steps]), agent_model.output_specs)\n",
    "agent_model.initialize(mock_env_outputs, mock_agent_outputs)\n",
    "\n",
    "explore_env_model.seed(42)\n",
    "for iteration in range(hparams.iterations):\n",
    "    if (iteration % hparams.eval_every) == 0:\n",
    "        (_, eval_env_outputs) = exploit_rollout().outputs\n",
    "        eval_returns = tf.reduce_sum(eval_env_outputs.reward * eval_env_outputs.weight, axis=1)\n",
    "        tf.print(tf.reduce_mean(eval_returns))\n",
    "\n",
    "    (agent_outputs, env_outputs) = explore_rollout().outputs\n",
    "    agent_value_outputs = agent_model.value(env_outputs, agent_outputs)\n",
    "    returns = discounted_returns(\n",
    "        env_outputs.reward * env_outputs.weight, \n",
    "        discounts=hparams.discounts)\n",
    "    advantages = generalized_advantage_estimate(\n",
    "        env_outputs.reward * env_outputs.weight, \n",
    "        agent_value_outputs.value * env_outputs.weight,\n",
    "        discounts=hparams.discounts, \n",
    "        lambdas=hparams.lambdas, \n",
    "        weights=env_outputs.weight)\n",
    "\n",
    "    for _ in range(hparams.epochs):\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(agent_model.trainable_variables)\n",
    "\n",
    "            agent_estimates_output = agent_model.policy_value(\n",
    "                env_outputs, agent_outputs)\n",
    "\n",
    "            ratio = tf.exp(\n",
    "                agent_estimates_output.log_prob - agent_outputs.log_prob)\n",
    "            surrogate1 = ratio * advantages\n",
    "            surrogate2 = tf.clip_by_value(\n",
    "                ratio,\n",
    "                1 - hparams.epsilon,\n",
    "                1 + hparams.epsilon) * advantages\n",
    "            surrogate_loss = tf.minimum(surrogate1, surrogate2)\n",
    "            policy_loss = -tf.reduce_sum(\n",
    "                surrogate_loss * env_outputs.weight)\n",
    "\n",
    "            value_loss = hparams.value_scale * tf.reduce_sum(\n",
    "                (tf.square(agent_estimates_output.value - tf.stop_gradient(returns)) *\n",
    "                 env_outputs.weight))\n",
    "\n",
    "            entropy_loss = -hparams.entropy_scale * tf.reduce_sum(\n",
    "                 agent_estimates_output.entropy * env_outputs.weight)\n",
    "\n",
    "            loss = (policy_loss + value_loss + entropy_loss) / (explore_size * max_steps)\n",
    "\n",
    "        variables = agent_model.trainable_variables\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "explore_env_model.close()\n",
    "exploit_env_model.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based On-Policy PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardOutput = collections.namedtuple(\n",
    "    'ForwardOutput', ['deltas_norm'])\n",
    "\n",
    "\n",
    "class CartPoleEnv(tf.Module):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(CartPoleEnv, self).__init__(name='CartPoleEnv')\n",
    "        self._hidden = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        self._logits = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(4)])\n",
    "        self.deltas_moments = pynr.moments.StreamingMoments([4])\n",
    "        self.output_specs = env.output_specs\n",
    "        self.output_shapes = tf.nest.map_structure(\n",
    "            lambda spec: spec.shape, self.output_specs)\n",
    "        self.output_dtypes = tf.nest.map_structure(\n",
    "            lambda spec: spec.dtype, self.output_specs)\n",
    "\n",
    "    @tf.function\n",
    "    def _scale_state(self, state):\n",
    "        state = (state / [[2.4, 10., 1., 10.]])\n",
    "        state = tf.concat(\n",
    "            [state, tf.stack([tf.math.cos(state[..., 2] / math.pi),\n",
    "                              tf.math.sin(state[..., 2] / math.pi)],\n",
    "                             axis=-1)],\n",
    "            axis=-1)\n",
    "        return tf.clip_by_value(state, -1., 1.)\n",
    "\n",
    "    @tf.function\n",
    "    def terminals(self, next_state, time_step):\n",
    "        is_terminal = pynr.debugging.mock_spec(\n",
    "            next_state.shape[:1], \n",
    "            self.output_specs.terminal, \n",
    "            tf.ones)\n",
    "\n",
    "        if time_step > 200:\n",
    "            return is_terminal\n",
    "\n",
    "        state_abs = tf.abs(next_state)\n",
    "        return tf.where(\n",
    "            tf.logical_or(tf.greater(state_abs[:, 0], 2.4), \n",
    "                          tf.greater(state_abs[:, 2], 12.)), \n",
    "            is_terminal, ~is_terminal)\n",
    "\n",
    "    @tf.function\n",
    "    def rewards(self, next_state):\n",
    "        return pynr.debugging.mock_spec(\n",
    "            next_state.shape[:1], \n",
    "            self.output_specs.reward, \n",
    "            tf.ones)\n",
    "\n",
    "    @tf.function\n",
    "    def forward(self, env_outputs, agent_outputs):\n",
    "        state = self._scale_state(env_outputs.state)\n",
    "        hidden = self._hidden(\n",
    "            tf.concat([\n",
    "                state, \n",
    "                tf.cast(agent_outputs.action[..., None], \n",
    "                        tf.dtypes.float32)\n",
    "            ], axis=-1))\n",
    "        deltas_norm = self._logits(hidden)\n",
    "        return ForwardOutput(deltas_norm=deltas_norm)\n",
    "\n",
    "    @tf.function\n",
    "    def reset(self, size, seed):\n",
    "        seed = tfp.distributions.SeedStream(seed, salt='forward_reset')\n",
    "        states_loc = [0., 0., 0., 0.]\n",
    "        states_scale_diag = [.25, .25, .25, .25]\n",
    "        initial_state_distribution = tfp.distributions.MultivariateNormalDiag(\n",
    "            loc=states_loc, scale_diag=states_scale_diag)\n",
    "        next_initial_state = initial_state_distribution.sample([size], seed=seed())\n",
    "        initial_state = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([size]), self.output_specs.state)\n",
    "        initial_reward = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([size]), self.output_specs.reward)\n",
    "        initial_terminal = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([size]), self.output_specs.terminal)\n",
    "        initial_weight = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([size]), self.output_specs.weight, \n",
    "            tf.ones)\n",
    "        return pyrl.rollouts.Transition(\n",
    "            state=initial_state, \n",
    "            next_state=next_initial_state,\n",
    "            reward=initial_reward,\n",
    "            terminal=initial_terminal,\n",
    "            weight=initial_weight)\n",
    "\n",
    "    @tf.function\n",
    "    def step(self, agent_outputs, env_outputs, time_step):\n",
    "        state = env_outputs.next_state\n",
    "        state = self._scale_state(state)\n",
    "        action = agent_outputs.action\n",
    "        hidden = self._hidden(\n",
    "            tf.concat([\n",
    "                state, \n",
    "                tf.cast(agent_outputs.action[..., None], \n",
    "                        tf.dtypes.float32)\n",
    "            ], axis=-1))\n",
    "        deltas_norm = self._logits(hidden)\n",
    "        deltas = self.deltas_moments.denormalize(\n",
    "            deltas_norm, env_outputs.weight[..., None])\n",
    "\n",
    "        delta_high = self.deltas_moments.mean + 3. * self.deltas_moments.std\n",
    "        deltas = tf.clip_by_value(deltas, -delta_high, delta_high)\n",
    "\n",
    "        next_state = env_outputs.next_state + deltas\n",
    "        terminal = self.terminals(next_state, time_step)\n",
    "        terminal = tf.logical_or(terminal, env_outputs.terminal)\n",
    "        reward = self.rewards(next_state)\n",
    "        weight = tf.cast(~env_outputs.terminal, tf.dtypes.float32)\n",
    "\n",
    "        return pyrl.rollouts.Transition(\n",
    "            state=env_outputs.next_state, \n",
    "            next_state=next_state,\n",
    "            reward=reward,\n",
    "            terminal=terminal,\n",
    "            weight=weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardModel(object):\n",
    "\n",
    "    def __init__(self, env, size):\n",
    "        self.env = env\n",
    "        self.size = size\n",
    "        self._seed = tf.Variable(0, trainable=False)\n",
    "\n",
    "    def seed(self, random_seed):\n",
    "        self._seed.assign(random_seed)\n",
    "\n",
    "    @tf.function\n",
    "    def reset(self, *args, **kwargs):\n",
    "        return self.env.reset(\n",
    "            *args, \n",
    "            size=self.size, \n",
    "            seed=self._seed,\n",
    "            **kwargs)\n",
    "\n",
    "    @tf.function\n",
    "    def step(self, *args, **kwargs):\n",
    "        return self.env.step(*args, **kwargs)\n",
    "\n",
    "ForwardHyperParameters = collections.namedtuple(\n",
    "    'ForwardHyperParameters', \n",
    "    ['iterations',\n",
    "     'epochs',\n",
    "     'learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward 202\n",
      "Real World 9.25\n",
      "Forward 202\n",
      "Real World 9.25\n",
      "Forward 202\n",
      "Real World 9.25\n",
      "Forward 202\n",
      "Real World 9.25\n",
      "Forward 202\n",
      "Real World 9.25\n",
      "Forward 202\n",
      "Real World 9.25\n",
      "Forward 202\n",
      "Real World 9.25\n",
      "Forward 202\n",
      "Real World 9.25\n",
      "Forward 202\n",
      "Real World 9.25\n",
      "Forward 202\n",
      "Real World 9.25\n",
      "Forward 94.3125\n",
      "Real World 9.5625\n",
      "Forward 83.8125\n",
      "Real World 9.5625\n",
      "Forward 86.9375\n",
      "Real World 9.5625\n",
      "Forward 95.1875\n",
      "Real World 9.5625\n",
      "Forward 88.3125\n",
      "Real World 9.5625\n",
      "Forward 88.8125\n",
      "Real World 9.5625\n",
      "Forward 94.25\n",
      "Real World 9.5625\n",
      "Forward 99.5\n",
      "Real World 9.5625\n",
      "Forward 96.5\n",
      "Real World 9.5625\n",
      "Forward 98.125\n",
      "Real World 9.5625\n",
      "Forward 66.6875\n",
      "Real World 9.5625\n",
      "Forward 75.125\n",
      "Real World 9.5625\n",
      "Forward 69.5625\n",
      "Real World 9.5625\n",
      "Forward 68.5\n",
      "Real World 9.5625\n",
      "Forward 68.8125\n",
      "Real World 9.5625\n",
      "Forward 68.5625\n",
      "Real World 9.5625\n",
      "Forward 67.4375\n",
      "Real World 9.5625\n",
      "Forward 71.375\n",
      "Real World 9.5625\n",
      "Forward 72.0625\n",
      "Real World 9.5625\n",
      "Forward 71\n",
      "Real World 9.5625\n",
      "Forward 65.9375\n",
      "Real World 9.5625\n",
      "Forward 65.1875\n",
      "Real World 9.5625\n",
      "Forward 66.5625\n",
      "Real World 9.5625\n",
      "Forward 64.25\n",
      "Real World 9.5625\n",
      "Forward 68.6875\n",
      "Real World 9.5625\n",
      "Forward 67.125\n",
      "Real World 9.5625\n",
      "Forward 66.25\n",
      "Real World 9.5625\n",
      "Forward 64.8125\n",
      "Real World 9.5625\n",
      "Forward 66.5\n",
      "Real World 9.5625\n",
      "Forward 69.625\n",
      "Real World 9.5625\n",
      "Forward 66.375\n",
      "Real World 9.5625\n",
      "Forward 62.3125\n",
      "Real World 9.5625\n",
      "Forward 63.6875\n",
      "Real World 9.5625\n",
      "Forward 63.125\n",
      "Real World 9.5625\n",
      "Forward 65.1875\n",
      "Real World 9.5625\n",
      "Forward 68.25\n",
      "Real World 9.5625\n",
      "Forward 65.3125\n",
      "Real World 9.5625\n",
      "Forward 64\n",
      "Real World 9.5625\n",
      "Forward 73.5625\n",
      "Real World 9.5625\n",
      "Forward 69.5\n",
      "Real World 9.5625\n",
      "Forward 123.1875\n",
      "Real World 9.5625\n",
      "Forward 98.1875\n",
      "Real World 9.5625\n",
      "Forward 113.3125\n",
      "Real World 9.5625\n",
      "Forward 118.1875\n",
      "Real World 9.5625\n",
      "Forward 79.75\n",
      "Real World 9.5625\n",
      "Forward 119.6875\n",
      "Real World 9.5625\n",
      "Forward 113.5\n",
      "Real World 9.625\n",
      "Forward 113.25\n",
      "Real World 9.5625\n",
      "Forward 119.5625\n",
      "Real World 9.5625\n",
      "Forward 113\n",
      "Real World 9.5625\n",
      "Forward 137.25\n",
      "Real World 9.5625\n",
      "Forward 142.375\n",
      "Real World 9.5625\n",
      "Forward 143\n",
      "Real World 9.5625\n",
      "Forward 123.875\n",
      "Real World 9.5625\n",
      "Forward 143.5625\n",
      "Real World 9.5625\n",
      "Forward 147.5\n",
      "Real World 9.5625\n",
      "Forward 152.4375\n",
      "Real World 9.5625\n",
      "Forward 140.75\n",
      "Real World 9.5625\n",
      "Forward 155.3125\n",
      "Real World 9.5625\n",
      "Forward 164.1875\n",
      "Real World 9.5625\n",
      "Forward 163.8125\n",
      "Real World 9.5625\n",
      "Forward 176.75\n",
      "Real World 9.5625\n",
      "Forward 167.8125\n",
      "Real World 9.5625\n",
      "Forward 178.3125\n",
      "Real World 9.5625\n",
      "Forward 162.25\n",
      "Real World 9.5625\n",
      "Forward 184.5\n",
      "Real World 9.5625\n",
      "Forward 193.125\n",
      "Real World 9.5625\n",
      "Forward 188.25\n",
      "Real World 9.5625\n",
      "Forward 195.8125\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.5625\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.6875\n",
      "Forward 202\n",
      "Real World 9.6875\n",
      "Forward 202\n",
      "Real World 9.625\n",
      "Forward 202\n",
      "Real World 9.75\n",
      "Forward 202\n",
      "Real World 9.75\n",
      "Forward 202\n",
      "Real World 9.875\n",
      "Forward 202\n",
      "Real World 9.9375\n",
      "Forward 202\n",
      "Real World 9.875\n",
      "Forward 202\n",
      "Real World 9.9375\n",
      "Forward 202\n",
      "Real World 9.9375\n",
      "Forward 202\n",
      "Real World 10.0625\n",
      "Forward 202\n",
      "Real World 9.9375\n",
      "Forward 202\n",
      "Real World 10.0625\n",
      "Forward 202\n",
      "Real World 10\n",
      "Forward 202\n",
      "Real World 10.0625\n",
      "Forward 202\n",
      "Real World 10.0625\n",
      "Forward 202\n",
      "Real World 10.0625\n",
      "Forward 202\n",
      "Real World 10.1875\n",
      "Forward 202\n",
      "Real World 10.1875\n",
      "Forward 202\n",
      "Real World 10.3125\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.625\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.625\n",
      "Forward 202\n",
      "Real World 10.3125\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.125\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.1875\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 9.9375\n",
      "Forward 202\n",
      "Real World 10.0625\n",
      "Forward 202\n",
      "Real World 9.9375\n",
      "Forward 202\n",
      "Real World 10.0625\n",
      "Forward 202\n",
      "Real World 9.9375\n",
      "Forward 202\n",
      "Real World 10.0625\n",
      "Forward 202\n",
      "Real World 9.9375\n",
      "Forward 202\n",
      "Real World 10.0625\n",
      "Forward 202\n",
      "Real World 9.9375\n",
      "Forward 202\n",
      "Real World 10.0625\n",
      "Forward 202\n",
      "Real World 9.9375\n",
      "Forward 202\n",
      "Real World 10\n",
      "Forward 202\n",
      "Real World 9.8125\n",
      "Forward 202\n",
      "Real World 10\n",
      "Forward 202\n",
      "Real World 9.75\n",
      "Forward 202\n",
      "Real World 9.9375\n",
      "Forward 202\n",
      "Real World 10\n",
      "Forward 202\n",
      "Real World 10\n",
      "Forward 202\n",
      "Real World 10\n",
      "Forward 202\n",
      "Real World 10\n",
      "Forward 202\n",
      "Real World 10\n",
      "Forward 202\n",
      "Real World 10\n",
      "Forward 202\n",
      "Real World 10.0625\n",
      "Forward 202\n",
      "Real World 10.0625\n",
      "Forward 202\n",
      "Real World 10.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward 202\n",
      "Real World 10.125\n",
      "Forward 202\n",
      "Real World 10.1875\n",
      "Forward 202\n",
      "Real World 10.1875\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.1875\n",
      "Forward 202\n",
      "Real World 10.125\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.1875\n",
      "Forward 202\n",
      "Real World 10.1875\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.125\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.5625\n",
      "Forward 202\n",
      "Real World 10.5625\n",
      "Forward 202\n",
      "Real World 10.5625\n",
      "Forward 202\n",
      "Real World 10.5\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.375\n",
      "Forward 202\n",
      "Real World 10.5625\n",
      "Forward 202\n",
      "Real World 10.4375\n",
      "Forward 202\n",
      "Real World 10.5625\n",
      "Forward 202\n",
      "Real World 10.4375\n"
     ]
    }
   ],
   "source": [
    "explore_size = 32\n",
    "exploit_size = 16\n",
    "forward_explore_size = 512\n",
    "forward_exploit_size = 16\n",
    "max_steps = 500\n",
    "\n",
    "explore_env_model = create_env_model(explore_size)\n",
    "exploit_env_model = create_env_model(exploit_size)\n",
    "\n",
    "forward_model = CartPoleEnv(explore_env_model)\n",
    "agent_model = CartPoleAgent(explore_env_model.action_spec)\n",
    "\n",
    "explore_strategy = Strategy(agent_model, True)\n",
    "exploit_strategy = Strategy(agent_model, False)\n",
    "\n",
    "explore_rollout = pyrl.rollouts.Rollout(explore_env_model, explore_strategy, max_steps)\n",
    "exploit_rollout = pyrl.rollouts.Rollout(exploit_env_model, exploit_strategy, max_steps)\n",
    "\n",
    "explore_forward_model = ForwardModel(forward_model, forward_explore_size)\n",
    "exploit_forward_model = ForwardModel(forward_model, forward_exploit_size)\n",
    "\n",
    "explore_forward_rollout = pyrl.rollouts.Rollout(explore_forward_model, explore_strategy, max_steps)\n",
    "exploit_forward_rollout = pyrl.rollouts.Rollout(exploit_forward_model, exploit_strategy, max_steps)\n",
    "\n",
    "hparams = HyperParameters(\n",
    "    iterations=10,\n",
    "    epochs=10,\n",
    "    discounts=.99,\n",
    "    lambdas=.975,\n",
    "    epsilon=.2,\n",
    "    value_scale=.5,\n",
    "    entropy_scale=.05,\n",
    "    eval_every=1,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "forward_hparams = ForwardHyperParameters(\n",
    "    iterations=30,\n",
    "    epochs=15,\n",
    "    learning_rate=1e-2,\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(hparams.learning_rate)\n",
    "forward_optimizer = tf.keras.optimizers.Adam(forward_hparams.learning_rate)\n",
    "\n",
    "discounted_returns = tf.function(pyrl.targets.discounted_returns)\n",
    "generalized_advantage_estimate = tf.function(pyrl.targets.generalized_advantage_estimate)\n",
    "\n",
    "explore_env_model.seed(42)\n",
    "explore_forward_model.seed(42)\n",
    "for forward_iteration in range(forward_hparams.iterations): \n",
    "    (agent_outputs, env_outputs) = explore_rollout().outputs\n",
    "\n",
    "    # Train the forward model\n",
    "    deltas = (env_outputs.next_state - env_outputs.state)\n",
    "    forward_model.deltas_moments.update_state(deltas, env_outputs.weight[..., None])\n",
    "    deltas_norm = forward_model.deltas_moments.normalize(\n",
    "        deltas, env_outputs.weight[..., None])\n",
    "\n",
    "    for _ in range(forward_hparams.epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            forward_estimates_output = forward_model.forward(\n",
    "                env_outputs, agent_outputs)\n",
    "            forward_loss = tf.reduce_mean(\n",
    "                tf.square(deltas_norm - forward_estimates_output.deltas_norm), \n",
    "                axis=-1)\n",
    "            loss = tf.reduce_sum(\n",
    "                forward_loss * env_outputs.weight)\n",
    "            loss = loss / (explore_size * max_steps)\n",
    "\n",
    "        variables = forward_model.trainable_variables\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        forward_optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "    # Train the policy\n",
    "    for iteration in range(hparams.iterations):\n",
    "        if (iteration % hparams.eval_every) == 0:\n",
    "            exploit_forward_model.seed(42 + forward_explore_size + 1)\n",
    "            (_, eval_forward_outputs) = exploit_forward_rollout().outputs\n",
    "            returns = tf.reduce_sum(eval_forward_outputs.reward * eval_forward_outputs.weight, axis=1)\n",
    "            tf.print('Forward', tf.reduce_mean(returns))\n",
    "\n",
    "            exploit_env_model.seed(42 + explore_size + 1)\n",
    "            (_, eval_env_outputs) = exploit_rollout().outputs\n",
    "            returns = tf.reduce_sum(eval_env_outputs.reward * eval_env_outputs.weight, axis=1)\n",
    "            tf.print('Real World', tf.reduce_mean(returns))\n",
    "\n",
    "        (agent_outputs, env_outputs) = explore_forward_rollout().outputs\n",
    "        agent_value_outputs = agent_model.value(env_outputs, agent_outputs)\n",
    "        returns = discounted_returns(\n",
    "            env_outputs.reward * env_outputs.weight, discounts=hparams.discounts)\n",
    "        advantages = generalized_advantage_estimate(\n",
    "            env_outputs.reward * env_outputs.weight, agent_value_outputs.value * env_outputs.weight,\n",
    "            discounts=hparams.discounts, lambdas=hparams.lambdas, weights=env_outputs.weight)\n",
    "\n",
    "        for _ in range(hparams.epochs):\n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                tape.watch(agent_model.trainable_variables)\n",
    "\n",
    "                agent_estimates_output = agent_model.policy_value(\n",
    "                    env_outputs, agent_outputs)\n",
    "\n",
    "                ratio = tf.exp(\n",
    "                    agent_estimates_output.log_prob - agent_outputs.log_prob)\n",
    "                surrogate1 = ratio * advantages\n",
    "                surrogate2 = tf.clip_by_value(\n",
    "                    ratio,\n",
    "                    1 - hparams.epsilon,\n",
    "                    1 + hparams.epsilon) * advantages\n",
    "                surrogate_loss = tf.minimum(surrogate1, surrogate2)\n",
    "                policy_loss = -tf.reduce_sum(\n",
    "                    surrogate_loss * env_outputs.weight)\n",
    "                value_loss = hparams.value_scale * tf.reduce_sum(\n",
    "                    (tf.square(agent_estimates_output.value - returns) *\n",
    "                     env_outputs.weight))\n",
    "                entropy_loss = -hparams.entropy_scale * tf.reduce_sum(\n",
    "                     agent_estimates_output.entropy * env_outputs.weight)\n",
    "                loss = (policy_loss + value_loss + entropy_loss) / (explore_size * max_steps)\n",
    "\n",
    "            variables = agent_model.trainable_variables\n",
    "            grads = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "explore_env_model.close()\n",
    "exploit_env_model.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On/Off-Policy IMPALA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor(host, port, actor_id, num_actors):\n",
    "    print('Starting Actor!')\n",
    "    max_steps = 500\n",
    "    explore_env_model = create_env_model()\n",
    "    agent_model = CartPoleAgent(explore_env_model.action_spec)\n",
    "    explore_strategy = Strategy(agent_model, True)\n",
    "    explore_rollout = pyrl.rollouts.Rollout(explore_env_model, explore_strategy, max_steps)\n",
    "\n",
    "    mock_env_outputs = pynr.debugging.mock_spec(\n",
    "        tf.TensorShape([1, max_steps]), explore_env_model.output_specs)\n",
    "    mock_agent_outputs = pynr.debugging.mock_spec(\n",
    "        tf.TensorShape([1, max_steps]), agent_model.output_specs)\n",
    "    agent_model.initialize(mock_env_outputs, mock_agent_outputs)\n",
    "\n",
    "    # Connect to the redis server.\n",
    "    pipe = redis.Redis(host=host, port=port, db=0)\n",
    "\n",
    "    # Control flow for rollouts.\n",
    "    cond = pynr.distributed.Condition(\n",
    "        pipe, 'WaitCondition')\n",
    "\n",
    "    # Queue for rollouts.\n",
    "    queue = pynr.distributed.Queue(\n",
    "        pipe, 'RolloutQueue',\n",
    "        dtypes=(tf.dtypes.int32, \n",
    "                (agent_model.output_dtypes, explore_env_model.output_dtypes)))\n",
    "\n",
    "    # Parameter server.\n",
    "    parameters = pynr.distributed.Register(\n",
    "        pipe, 'Parameters',\n",
    "        dtypes=tf.nest.map_structure(lambda var: var.dtype,\n",
    "                                     agent_model.variables))\n",
    "\n",
    "    # Queried to determine when to sync parameters.\n",
    "    sync = pynr.distributed.MultiEvent(\n",
    "        pipe, actor_id, num_actors, 'SyncParameters')\n",
    "\n",
    "    explore_env_model.seed(42 + actor_id)\n",
    "    while True:\n",
    "        cond.wait(actor_id)\n",
    "        # Sync parameters only if we need to.\n",
    "        if sync.get():\n",
    "            sync.unset()\n",
    "            tf.nest.map_structure(\n",
    "                lambda dst, src: dst.assign(src),\n",
    "                agent_model.variables,\n",
    "                parameters.get())\n",
    "        values = explore_rollout().outputs\n",
    "        queue.enqueue((tf.cast(actor_id, tf.dtypes.int32), \n",
    "                       values))\n",
    "\n",
    "\n",
    "def learner(host, port, num_actors):\n",
    "    batch_size = 32\n",
    "    exploit_size = 16\n",
    "    max_steps = 500\n",
    "    exploit_env_model = create_env_model()\n",
    "    agent_model = CartPoleAgent(exploit_env_model.action_spec)\n",
    "    exploit_strategy = Strategy(agent_model, False)\n",
    "    exploit_rollout = pyrl.rollouts.Rollout(exploit_env_model, exploit_strategy, max_steps)\n",
    "\n",
    "    mock_env_outputs = pynr.debugging.mock_spec(\n",
    "        tf.TensorShape([1, max_steps]), exploit_env_model.output_specs)\n",
    "    mock_agent_outputs = pynr.debugging.mock_spec(\n",
    "        tf.TensorShape([1, max_steps]), agent_model.output_specs)\n",
    "    agent_model.initialize(mock_env_outputs, mock_agent_outputs)\n",
    "\n",
    "    hparams = HyperParameters(\n",
    "        iterations=100,\n",
    "        discounts=.99,\n",
    "        value_scale=.5,\n",
    "        epochs=None,\n",
    "        lambdas=None,\n",
    "        epsilon=None,\n",
    "        entropy_scale=.05,\n",
    "        eval_every=1,\n",
    "        learning_rate=1e-3,\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(hparams.learning_rate)\n",
    "    v_trace_returns = tf.function(pyrl.targets.v_trace_returns)\n",
    "    temporal_difference = tf.function(pyrl.targets.temporal_difference)\n",
    "\n",
    "    # Connect to the redis server.\n",
    "    pipe = redis.Redis(host=host, port=port, db=0)\n",
    "\n",
    "    # Control flow for rollouts.\n",
    "    cond = pynr.distributed.Condition(\n",
    "        pipe, 'WaitCondition')\n",
    "\n",
    "    # Queue for rollouts.\n",
    "    queue = pynr.distributed.Queue(\n",
    "        pipe, 'RolloutQueue',\n",
    "        dtypes=(tf.dtypes.int32, \n",
    "                (agent_model.output_dtypes, exploit_env_model.output_dtypes)))\n",
    "\n",
    "    # Parameter server.\n",
    "    parameters = pynr.distributed.Register(\n",
    "        pipe, 'Parameters',\n",
    "        dtypes=tf.nest.map_structure(lambda var: var.dtype,\n",
    "                                     agent_model.variables))\n",
    "\n",
    "    # Queried to determine when to sync parameters.\n",
    "    sync = pynr.distributed.MultiEvent(\n",
    "        pipe, num_actors, num_actors, 'SyncParameters')\n",
    "\n",
    "    def reader_fn():\n",
    "        while True:\n",
    "            actor_id, values = queue.dequeue()\n",
    "            cond.notify(actor_id)\n",
    "            yield tf.nest.map_structure(\n",
    "                lambda t: tf.squeeze(t, axis=0), values)\n",
    "\n",
    "    def set_n_step_shape_fn(shape):\n",
    "        return tf.TensorShape([max_steps] + shape.as_list())\n",
    "\n",
    "    agent_output_shapes = tf.nest.map_structure(\n",
    "        set_n_step_shape_fn, agent_model.output_shapes)\n",
    "    env_output_shapes = tf.nest.map_structure(\n",
    "        set_n_step_shape_fn, exploit_env_model.output_shapes)\n",
    "\n",
    "    # Stage N batches ahead of time\n",
    "    prefetch_size = 1\n",
    "    actor = tf.data.Dataset.from_generator(\n",
    "        reader_fn,\n",
    "        output_types=(agent_model.output_dtypes, exploit_env_model.output_dtypes),\n",
    "        output_shapes=(agent_output_shapes, env_output_shapes))\n",
    "    actor = actor.batch(batch_size)\n",
    "    actor = actor.prefetch(prefetch_size)\n",
    "    actor_reader = iter(actor)\n",
    "    \n",
    "    print('Starting learner!')\n",
    "    for iteration in range(hparams.iterations):\n",
    "        parameters.set(agent_model.variables)\n",
    "        sync.set_all()\n",
    "        cond.notify_all()\n",
    "\n",
    "        if (iteration % hparams.eval_every) == 0:\n",
    "            exploit_env_model.seed(42 + num_actors + 1)\n",
    "            (_, eval_env_outputs) = exploit_rollout().outputs\n",
    "            returns = tf.reduce_sum(eval_env_outputs.reward * eval_env_outputs.weight, axis=1)\n",
    "            tf.print(tf.reduce_mean(returns))\n",
    "\n",
    "        (agent_outputs, env_outputs) = next(actor_reader)\n",
    "\n",
    "        # Estimate gradients here.\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(agent_model.trainable_variables)\n",
    "\n",
    "            (agent_estimates_output,\n",
    "             agent_value_output_last) = agent_model.policy_value_with_nexts(\n",
    "                 env_outputs, agent_outputs)\n",
    "\n",
    "            masked_discounts = tf.cast(\n",
    "                ~env_outputs.terminal, tf.dtypes.float32) * hparams.discounts\n",
    "            returns = v_trace_returns(\n",
    "                env_outputs.reward * env_outputs.weight,\n",
    "                agent_estimates_output.value * env_outputs.weight,\n",
    "                agent_estimates_output.log_prob * env_outputs.weight,\n",
    "                agent_outputs.log_prob * env_outputs.weight,\n",
    "                last_value=agent_value_output_last.value,\n",
    "                discounts=masked_discounts,\n",
    "                weights=env_outputs.weight)\n",
    "\n",
    "            returns_next = tf.concat(\n",
    "                [returns[:, 1:], tf.expand_dims(\n",
    "                    agent_value_output_last.value, axis=1)],\n",
    "                axis=1)\n",
    "\n",
    "            clipped_is = tf.math.minimum(\n",
    "                1., tf.exp(\n",
    "                    agent_estimates_output.log_prob - agent_outputs.log_prob))\n",
    "            clipped_is = tf.stop_gradient(clipped_is)\n",
    "            returns = tf.stop_gradient(\n",
    "                env_outputs.reward + masked_discounts * returns_next)\n",
    "\n",
    "            advantages = clipped_is * temporal_difference(\n",
    "                returns * env_outputs.weight,\n",
    "                agent_estimates_output.value * env_outputs.weight,\n",
    "                back_prop=True)\n",
    "\n",
    "            policy_loss = -tf.reduce_sum(\n",
    "                (agent_estimates_output.log_prob * advantages) *\n",
    "                env_outputs.weight)\n",
    "            value_loss = hparams.value_scale * tf.reduce_sum(\n",
    "                (tf.square(advantages) * env_outputs.weight))\n",
    "            entropy_loss = -hparams.entropy_scale * tf.reduce_sum(\n",
    "                agent_estimates_output.entropy * env_outputs.weight)\n",
    "            loss = ((policy_loss + value_loss + entropy_loss) /\n",
    "                    (batch_size * max_steps))\n",
    "\n",
    "        variables = agent_model.trainable_variables\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "    explore_env_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Actor!\n",
      "Starting Actor!\n"
     ]
    }
   ],
   "source": [
    "num_actors = 2\n",
    "host = '127.0.0.1'\n",
    "port = '6380'\n",
    "\n",
    "actor_processes = []\n",
    "for actor_id in range(num_actors):\n",
    "    p = multiprocessing.Process(target=actor, args=(host, port, actor_id, num_actors,))\n",
    "    p.start()\n",
    "    actor_processes.append(p)\n",
    "\n",
    "time.sleep(2)\n",
    "learner_proc = multiprocessing.Process(target=learner, args=(host, port, num_actors,))\n",
    "learner_proc.start()\n",
    "learner_proc.join()\n",
    "for p in actor_processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyoneer-K_ZVJbe4",
   "language": "python",
   "name": "pyoneer-k_zvjbe4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
