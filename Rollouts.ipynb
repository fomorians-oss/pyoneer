{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import math\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pyoneer as pynr\n",
    "import pyoneer.rl as pyrl\n",
    "\n",
    "# Seed the environment.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_env_model(batch_size=None):\n",
    "    # Create the gym env, wrapped in a vectorized manner.\n",
    "    env_spec = 'CartPole-v1'\n",
    "\n",
    "    def make_fn():\n",
    "        env = gym.make(env_spec)\n",
    "        env.observation_space.dtype = 'float64'\n",
    "        return env\n",
    "\n",
    "    if batch_size is None:\n",
    "        gym_env = make_fn()\n",
    "    else:\n",
    "        gym_env = pyrl.wrappers.Batch(make_fn, batch_size)\n",
    "\n",
    "    # Wrap it in a Model.\n",
    "    env_model = pyrl.rollouts.Env(gym_env)\n",
    "    return env_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.01138523 -0.04794491 -0.01941054  0.01141418]], shape=(1, 4), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "env_model = create_env_model(1)\n",
    "env_outputs = env_model.reset()\n",
    "print(env_outputs.next_state)\n",
    "env_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentPolicyOutput = collections.namedtuple(\n",
    "    'AgentPolicyOutput', ['action'])\n",
    "AgentValueOutput = collections.namedtuple(\n",
    "    'AgentValueOutput', ['value'])\n",
    "AgentPolicyValueOutput = collections.namedtuple(\n",
    "    'AgentPolicyValueOutput', ['log_prob', 'entropy', 'value'])\n",
    "\n",
    "\n",
    "class CartPoleAgent(tf.Module):\n",
    "\n",
    "    def __init__(self, action_spec):\n",
    "        super(CartPoleAgent, self).__init__(name='CartPoleAgent')\n",
    "        self._hidden = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        self._logits = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(2)])\n",
    "        self._value = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(1)])\n",
    "        self._policy = tfp.distributions.Categorical\n",
    "\n",
    "        self.action_spec = action_spec\n",
    "        self.output_specs = AgentPolicyOutput(\n",
    "            action=self.action_spec)\n",
    "        self.output_shapes = tf.nest.map_structure(\n",
    "            lambda spec: spec.shape, self.output_specs)\n",
    "        self.output_dtypes = tf.nest.map_structure(\n",
    "            lambda spec: spec.dtype, self.output_specs)\n",
    "\n",
    "    @property\n",
    "    def value_trainable_variables(self):\n",
    "        return self._hidden.trainable_variables + self._value.trainable_variables\n",
    "\n",
    "    @property\n",
    "    def policy_trainable_variables(self):\n",
    "        return self._hidden.trainable_variables + self._logits.trainable_variables\n",
    "\n",
    "    @tf.function\n",
    "    def _scale_state(self, state):\n",
    "        state = tf.cast(state, tf.float32)\n",
    "        state = (state / [[2.4, 10., 1., 10.]])\n",
    "        state = tf.concat(\n",
    "            [state, tf.stack([tf.math.cos(state[..., 2] / math.pi),\n",
    "                              tf.math.sin(state[..., 2] / math.pi)],\n",
    "                             axis=-1)],\n",
    "            axis=-1)\n",
    "        return tf.clip_by_value(state, -1., 1.)\n",
    "\n",
    "    @tf.function\n",
    "    def initialize(self, env_outputs, agent_outputs):\n",
    "        state = self._scale_state(env_outputs.state)\n",
    "        hidden = self._hidden(state)\n",
    "        _ = self._value(hidden)\n",
    "        _ = self._logits(hidden)\n",
    "    \n",
    "    @tf.function\n",
    "    def value(self, env_outputs, agent_outputs):\n",
    "        state = self._scale_state(env_outputs.state)\n",
    "        hidden = self._hidden(state)\n",
    "        value = tf.squeeze(self._value(hidden), axis=-1)\n",
    "        return AgentValueOutput(value=value)\n",
    "\n",
    "    @tf.function\n",
    "    def policy_value(self, env_outputs, agent_outputs):\n",
    "        state = self._scale_state(env_outputs.state)\n",
    "        hidden = self._hidden(state)\n",
    "        logits = self._logits(hidden)\n",
    "        policy = self._policy(logits=logits)\n",
    "        entropy = policy.entropy()\n",
    "        log_prob = policy.log_prob(agent_outputs.action)\n",
    "        value = tf.squeeze(self._value(hidden), axis=-1)\n",
    "        return AgentPolicyValueOutput(log_prob=log_prob,\n",
    "                                      entropy=entropy,\n",
    "                                      value=value)\n",
    "\n",
    "    @tf.function\n",
    "    def reset(self, env_outputs, explore=True):\n",
    "        initial_action = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([env_outputs.state.shape[0]]), \n",
    "            self.action_spec, \n",
    "            tf.zeros)\n",
    "        return AgentPolicyOutput(\n",
    "            action=initial_action)\n",
    "\n",
    "    @tf.function\n",
    "    def step(self, env_outputs, agent_outputs, time_step, explore=True):\n",
    "        state = env_outputs.next_state\n",
    "        state = self._scale_state(state)\n",
    "        hidden = self._hidden(state)\n",
    "        logits = self._logits(hidden)\n",
    "        policy = self._policy(logits=logits)\n",
    "\n",
    "        if explore:\n",
    "            action = policy.sample()\n",
    "        else:\n",
    "            action = policy.mode()\n",
    "\n",
    "        action = tf.nest.map_structure(\n",
    "            lambda t, s: tf.cast(t, s.dtype), \n",
    "            action, self.action_spec)\n",
    "        return AgentPolicyOutput(action=action)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy(object):\n",
    "\n",
    "    def __init__(self, agent, explore):\n",
    "        self.agent = agent\n",
    "        self.explore = explore\n",
    "\n",
    "    @tf.function\n",
    "    def reset(self, *args, **kwargs):\n",
    "        return self.agent.reset(*args, explore=self.explore, **kwargs)\n",
    "\n",
    "    @tf.function\n",
    "    def step(self, *args, **kwargs):\n",
    "        return self.agent.step(*args, explore=self.explore, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-Policy AWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3515625\n",
      "9.4765625\n",
      "9.421875\n",
      "9.3515625\n",
      "9.4140625\n",
      "9.4453125\n",
      "9.375\n",
      "9.3515625\n",
      "9.421875\n",
      "9.3046875\n",
      "9.3203125\n",
      "9.2578125\n",
      "9.234375\n",
      "9.375\n",
      "9.2421875\n",
      "9.3203125\n",
      "9.3125\n",
      "9.296875\n",
      "9.3671875\n",
      "9.3203125\n",
      "9.375\n",
      "9.3828125\n",
      "9.4296875\n",
      "9.359375\n",
      "9.515625\n",
      "9.3984375\n",
      "9.4140625\n",
      "9.3203125\n",
      "9.421875\n",
      "9.453125\n",
      "9.4296875\n",
      "9.3828125\n",
      "9.3046875\n",
      "9.40625\n",
      "9.25\n",
      "9.234375\n",
      "9.2734375\n",
      "9.3984375\n",
      "9.46875\n",
      "9.421875\n",
      "9.34375\n",
      "9.3125\n",
      "9.390625\n",
      "9.296875\n",
      "9.359375\n",
      "9.390625\n",
      "9.375\n",
      "9.2890625\n",
      "9.3515625\n",
      "9.3671875\n",
      "9.3359375\n",
      "9.3828125\n",
      "9.2421875\n",
      "9.3984375\n",
      "9.5625\n",
      "10.0546875\n",
      "11.625\n",
      "14.78125\n",
      "23.9921875\n",
      "34.2578125\n",
      "29.515625\n",
      "36.859375\n",
      "47.140625\n",
      "54.6484375\n",
      "55.8515625\n",
      "59.703125\n",
      "65.9140625\n",
      "68.625\n",
      "72.8984375\n",
      "73.2578125\n",
      "73.8984375\n",
      "81.5078125\n",
      "77.1796875\n",
      "72.3671875\n",
      "67.96875\n",
      "76.84375\n",
      "102.453125\n",
      "144.242188\n",
      "115.492188\n",
      "89.3359375\n",
      "97.7109375\n",
      "114.679688\n",
      "123.414062\n",
      "103.476562\n",
      "90.5703125\n",
      "101.703125\n",
      "120.664062\n",
      "109.25\n",
      "98.9375\n",
      "95.4375\n",
      "96.078125\n",
      "121.117188\n",
      "102.257812\n",
      "92.3515625\n",
      "92.7890625\n",
      "102.171875\n",
      "114.796875\n",
      "104.742188\n"
     ]
    }
   ],
   "source": [
    "HyperParameters = collections.namedtuple(\n",
    "    'HyperParameters', \n",
    "    ['iterations',\n",
    "     'value_steps',\n",
    "     'policy_steps',\n",
    "     'discounts',\n",
    "     'lambdas',\n",
    "     'beta',\n",
    "     'score_max',\n",
    "     'value_scale',\n",
    "     'eval_every',\n",
    "     'learning_rate'])\n",
    "\n",
    "explore_size = 128\n",
    "exploit_size = 100\n",
    "max_steps = 500\n",
    "\n",
    "explore_env_model = create_env_model(explore_size)\n",
    "exploit_env_model = create_env_model(explore_size)\n",
    "\n",
    "agent_model = CartPoleAgent(explore_env_model.action_spec)\n",
    "\n",
    "explore_strategy = Strategy(agent_model, True)\n",
    "exploit_strategy = Strategy(agent_model, False)\n",
    "\n",
    "explore_rollout = pyrl.rollouts.Rollout(explore_env_model, explore_strategy, max_steps)\n",
    "exploit_rollout = pyrl.rollouts.Rollout(exploit_env_model, exploit_strategy, max_steps)\n",
    "\n",
    "hparams = HyperParameters(\n",
    "    iterations=100,\n",
    "    value_steps=50,\n",
    "    policy_steps=10,\n",
    "    discounts=.99,\n",
    "    lambdas=.95,\n",
    "    beta=.05,\n",
    "    score_max=100.,\n",
    "    value_scale=.5,\n",
    "    eval_every=1,\n",
    "    learning_rate=1e-4,\n",
    ")\n",
    "value_optimizer = tf.keras.optimizers.Adam(hparams.learning_rate)\n",
    "policy_optimizer = tf.keras.optimizers.Adam(hparams.learning_rate)\n",
    "\n",
    "discounted_returns = tf.function(pyrl.targets.discounted_returns)\n",
    "generalized_advantage_estimate = tf.function(pyrl.targets.generalized_advantage_estimate)\n",
    "\n",
    "mock_env_outputs = pynr.debugging.mock_spec(\n",
    "    tf.TensorShape([1, max_steps]), explore_env_model.output_specs, tf.zeros)\n",
    "mock_agent_outputs = pynr.debugging.mock_spec(\n",
    "    tf.TensorShape([1, max_steps]), agent_model.output_specs, tf.zeros)\n",
    "agent_model.initialize(mock_env_outputs, mock_agent_outputs)\n",
    "\n",
    "explore_env_model.seed(42)\n",
    "for iteration in range(hparams.iterations):\n",
    "    if (iteration % hparams.eval_every) == 0:\n",
    "        (_, eval_env_outputs) = exploit_rollout().outputs\n",
    "        eval_returns = tf.reduce_sum(\n",
    "            tf.cast(eval_env_outputs.reward, tf.float32) * eval_env_outputs.weight, \n",
    "            axis=1)\n",
    "        tf.print(tf.reduce_mean(eval_returns))\n",
    "\n",
    "    (agent_outputs, env_outputs) = explore_rollout().outputs\n",
    "    returns = discounted_returns(\n",
    "        tf.cast(env_outputs.reward, tf.float32) * env_outputs.weight, \n",
    "        discounts=hparams.discounts)\n",
    "    \n",
    "    for _ in range(hparams.value_steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            agent_value_outputs = agent_model.value(env_outputs, agent_outputs)\n",
    "            value_loss = hparams.value_scale * tf.reduce_sum(\n",
    "                (tf.square(agent_value_outputs.value - tf.stop_gradient(returns)) *\n",
    "                 env_outputs.weight))\n",
    "            loss = value_loss / (explore_size * max_steps)\n",
    "\n",
    "        variables = agent_model.value_trainable_variables\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        value_optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "    agent_value_outputs = agent_model.value(env_outputs, agent_outputs)\n",
    "    advantages = generalized_advantage_estimate(\n",
    "        tf.cast(env_outputs.reward, tf.float32) * env_outputs.weight, \n",
    "        agent_value_outputs.value * env_outputs.weight,\n",
    "        discounts=hparams.discounts, \n",
    "        lambdas=hparams.lambdas, \n",
    "        weights=env_outputs.weight)\n",
    "\n",
    "    for _ in range(hparams.policy_steps):\n",
    "        with tf.GradientTape() as tape:\n",
    "            agent_estimates_output = agent_model.policy_value(\n",
    "                env_outputs, agent_outputs)\n",
    "            score = tf.minimum(tf.exp(advantages / hparams.beta), hparams.score_max)\n",
    "            policy_loss = -tf.reduce_sum(\n",
    "                agent_estimates_output.log_prob * tf.stop_gradient(score) * env_outputs.weight)\n",
    "            loss = policy_loss / (explore_size * max_steps)\n",
    "\n",
    "        variables = agent_model.policy_trainable_variables\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        policy_optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "explore_env_model.close()\n",
    "exploit_env_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyoneer-K_ZVJbe4",
   "language": "python",
   "name": "pyoneer-k_zvjbe4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
