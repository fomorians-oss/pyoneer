{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import functools\n",
    "import math\n",
    "import gym\n",
    "import redis\n",
    "import time\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import pyoneer as pynr\n",
    "import pyoneer.rl as pyrl\n",
    "\n",
    "# Seed the environment.\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_env_model(batch_size=None):\n",
    "    # Create the gym env, wrapped in a vectorized manner.\n",
    "    env_spec = 'CartPole-v1'\n",
    "\n",
    "    if batch_size is None:\n",
    "        gym_env = gym.make(env_spec)\n",
    "    else:\n",
    "        gym_env = pyrl.wrappers.Batch(lambda: gym.make(env_spec), batch_size)\n",
    "\n",
    "    # Wrap it in a Model.\n",
    "    env_model = pyrl.rollouts.Env(gym_env)\n",
    "    return env_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_model = create_env_model(1)\n",
    "env_outputs = env_model.reset()\n",
    "print(env_outputs.next_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions\n",
    "- On-policy PPO\n",
    "- Model-based, on-policy PPO\n",
    "- On/Off-policy IMPALA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "AgentPolicyOutput = collections.namedtuple(\n",
    "    'AgentPolicyOutput', ['action', 'log_prob'])\n",
    "AgentValueOutput = collections.namedtuple(\n",
    "    'AgentValueOutput', ['value'])\n",
    "AgentPolicyValueOutput = collections.namedtuple(\n",
    "    'AgentPolicyValueOutput', ['log_prob', 'entropy', 'value'])\n",
    "\n",
    "\n",
    "class CartPoleAgent(tf.Module):\n",
    "\n",
    "    def __init__(self, action_spec):\n",
    "        super(CartPoleAgent, self).__init__(name='CartPoleAgent')\n",
    "        self._hidden = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        self._logits = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(2)])\n",
    "        self._value = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(1)])\n",
    "        self._policy = tfp.distributions.Categorical\n",
    "\n",
    "        self.action_spec = action_spec\n",
    "        self.log_prob_spec = tf.nest.map_structure(\n",
    "            lambda spec: tf.TensorSpec([], tf.dtypes.float32),\n",
    "            self.action_spec)\n",
    "        self.output_specs = AgentPolicyOutput(\n",
    "            action=self.action_spec,\n",
    "            log_prob=self.log_prob_spec)\n",
    "        self.output_shapes = tf.nest.map_structure(\n",
    "            lambda spec: spec.shape, self.output_specs)\n",
    "        self.output_dtypes = tf.nest.map_structure(\n",
    "            lambda spec: spec.dtype, self.output_specs)\n",
    "\n",
    "    @tf.function\n",
    "    def _scale_state(self, state):\n",
    "        state = (state / [[2.4, 10., 1., 10.]])\n",
    "        state = tf.concat(\n",
    "            [state, tf.stack([tf.math.cos(state[..., 2] / math.pi),\n",
    "                              tf.math.sin(state[..., 2] / math.pi)],\n",
    "                             axis=-1)],\n",
    "            axis=-1)\n",
    "        return tf.clip_by_value(state, -1., 1.)\n",
    "\n",
    "    @tf.function\n",
    "    def initialize(self, env_outputs, agent_outputs):\n",
    "        state = self._scale_state(env_outputs.state)\n",
    "        hidden = self._hidden(state)\n",
    "        _ = self._value(hidden)\n",
    "        _ = self._logits(hidden)\n",
    "    \n",
    "    @tf.function\n",
    "    def value(self, env_outputs, agent_outputs):\n",
    "        state = self._scale_state(env_outputs.state)\n",
    "        hidden = self._hidden(state)\n",
    "        value = tf.squeeze(self._value(hidden), axis=-1)\n",
    "        return AgentValueOutput(value=value)\n",
    "\n",
    "    @tf.function\n",
    "    def policy_value(self, env_outputs, agent_outputs):\n",
    "        state = self._scale_state(env_outputs.state)\n",
    "        hidden = self._hidden(state)\n",
    "        logits = self._logits(hidden)\n",
    "        policy = self._policy(logits=logits)\n",
    "        entropy = policy.entropy()\n",
    "        log_prob = policy.log_prob(agent_outputs.action)\n",
    "        value = tf.squeeze(self._value(hidden), axis=-1)\n",
    "        return AgentPolicyValueOutput(log_prob=log_prob,\n",
    "                                      entropy=entropy,\n",
    "                                      value=value)\n",
    "\n",
    "    @tf.function\n",
    "    def policy_value_with_nexts(self, env_outputs, agent_outputs):\n",
    "        # Add bootstrap state\n",
    "        def bootstrap_state(s_t, s_tp1):\n",
    "            return tf.concat([s_t, s_tp1[:, -1:]], axis=1)\n",
    "\n",
    "        bootstrapped_state = tf.nest.map_structure(\n",
    "            bootstrap_state, env_outputs.state, env_outputs.next_state)\n",
    "\n",
    "        state = self._scale_state(bootstrapped_state)\n",
    "        hidden = self._hidden(state)\n",
    "        value = tf.squeeze(self._value(hidden), axis=-1)\n",
    "\n",
    "        logits = self._logits(hidden[:, :-1])\n",
    "        policy = self._policy(logits=logits)\n",
    "        log_prob = policy.log_prob(agent_outputs.action)\n",
    "        entropy = policy.entropy()\n",
    "\n",
    "        outputs = AgentPolicyValueOutput(log_prob=log_prob,\n",
    "                                         entropy=entropy,\n",
    "                                         value=value[:, :-1])\n",
    "        bootstrap = AgentValueOutput(value=value[:, -1])\n",
    "        return outputs, bootstrap\n",
    "\n",
    "    @tf.function\n",
    "    def reset(self, env_outputs, explore=False):\n",
    "        initial_action = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([env_outputs.state.shape[0]]), \n",
    "            self.action_spec, \n",
    "            tf.zeros)\n",
    "        initial_log_prob = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([env_outputs.state.shape[0]]), \n",
    "            self.log_prob_spec, \n",
    "            tf.zeros)\n",
    "        return AgentPolicyOutput(\n",
    "            action=initial_action,\n",
    "            log_prob=initial_log_prob)\n",
    "\n",
    "    @tf.function\n",
    "    def step(self, env_outputs, agent_outputs, time_step, explore=False):\n",
    "        state = env_outputs.next_state\n",
    "        state = self._scale_state(state)\n",
    "        hidden = self._hidden(state)\n",
    "        logits = self._logits(hidden)\n",
    "        policy = self._policy(logits=logits)\n",
    "\n",
    "        if explore:\n",
    "            action = policy.sample()\n",
    "        else:\n",
    "            action = policy.mode()\n",
    "\n",
    "        action = tf.nest.map_structure(\n",
    "            lambda t, s: tf.cast(t, s.dtype), \n",
    "            action, self.action_spec)\n",
    "        log_prob = policy.log_prob(action)\n",
    "        return AgentPolicyOutput(action=action,\n",
    "                                 log_prob=log_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Strategy(object):\n",
    "\n",
    "    def __init__(self, agent, explore):\n",
    "        self.agent = agent\n",
    "        self.explore = explore\n",
    "\n",
    "    @tf.function\n",
    "    def reset(self, *args, **kwargs):\n",
    "        return self.agent.reset(*args, explore=self.explore, **kwargs)\n",
    "\n",
    "    @tf.function\n",
    "    def step(self, *args, **kwargs):\n",
    "        return self.agent.step(*args, explore=self.explore, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HyperParameters = collections.namedtuple(\n",
    "    'HyperParameters', \n",
    "    ['iterations',\n",
    "     'epochs',\n",
    "     'discounts',\n",
    "     'lambdas',\n",
    "     'epsilon',\n",
    "     'value_scale',\n",
    "     'entropy_scale',\n",
    "     'eval_every',\n",
    "     'learning_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On-Policy PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_size = 128\n",
    "exploit_size = 16\n",
    "max_steps = 500\n",
    "\n",
    "explore_env_model = create_env_model(explore_size)\n",
    "exploit_env_model = create_env_model(exploit_size)\n",
    "\n",
    "agent_model = CartPoleAgent(explore_env_model.action_spec)\n",
    "\n",
    "explore_strategy = Strategy(agent_model, True)\n",
    "exploit_strategy = Strategy(agent_model, False)\n",
    "\n",
    "explore_rollout = pyrl.rollouts.Rollout(explore_env_model, explore_strategy, max_steps)\n",
    "exploit_rollout = pyrl.rollouts.Rollout(exploit_env_model, exploit_strategy, max_steps)\n",
    "\n",
    "hparams = HyperParameters(\n",
    "    # iterations=500,\n",
    "    iterations=1,\n",
    "    epochs=5,\n",
    "    discounts=.99,\n",
    "    lambdas=.975,\n",
    "    epsilon=.2,\n",
    "    value_scale=.5,\n",
    "    entropy_scale=.05,\n",
    "    eval_every=10,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(hparams.learning_rate)\n",
    "discounted_returns = tf.function(pyrl.targets.discounted_returns)\n",
    "generalized_advantage_estimate = tf.function(pyrl.targets.generalized_advantage_estimate)\n",
    "\n",
    "mock_env_outputs = pynr.debugging.mock_spec(\n",
    "    tf.TensorShape([1, max_steps]), explore_env_model.output_specs)\n",
    "mock_agent_outputs = pynr.debugging.mock_spec(\n",
    "    tf.TensorShape([1, max_steps]), agent_model.output_specs)\n",
    "agent_model.initialize(mock_env_outputs, mock_agent_outputs)\n",
    "\n",
    "explore_env_model.seed(42)\n",
    "for iteration in range(hparams.iterations):\n",
    "    if (iteration % hparams.eval_every) == 0:\n",
    "        exploit_env_model.seed(42 + explore_size + 1)\n",
    "        (_, eval_env_outputs) = exploit_rollout().outputs\n",
    "        eval_returns = tf.reduce_sum(eval_env_outputs.reward * eval_env_outputs.weight, axis=1)\n",
    "        tf.print(tf.reduce_mean(eval_returns))\n",
    "\n",
    "    (agent_outputs, env_outputs) = explore_rollout().outputs\n",
    "    agent_value_outputs = agent_model.value(env_outputs, agent_outputs)\n",
    "    returns = discounted_returns(\n",
    "        env_outputs.reward * env_outputs.weight, discounts=hparams.discounts)\n",
    "    advantages = generalized_advantage_estimate(\n",
    "        env_outputs.reward * env_outputs.weight, agent_value_outputs.value * env_outputs.weight,\n",
    "        discounts=hparams.discounts, lambdas=hparams.lambdas, weights=env_outputs.weight)\n",
    "\n",
    "    for _ in range(hparams.epochs):\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(agent_model.trainable_variables)\n",
    "\n",
    "            agent_estimates_output = agent_model.policy_value(\n",
    "                env_outputs, agent_outputs)\n",
    "\n",
    "            ratio = tf.exp(\n",
    "                agent_estimates_output.log_prob - agent_outputs.log_prob)\n",
    "            surrogate1 = ratio * advantages\n",
    "            surrogate2 = tf.clip_by_value(\n",
    "                ratio,\n",
    "                1 - hparams.epsilon,\n",
    "                1 + hparams.epsilon) * advantages\n",
    "            surrogate_loss = tf.minimum(surrogate1, surrogate2)\n",
    "            policy_loss = -tf.reduce_sum(\n",
    "                surrogate_loss * env_outputs.weight)\n",
    "            value_loss = hparams.value_scale * tf.reduce_sum(\n",
    "                (tf.square(agent_estimates_output.value - returns) *\n",
    "                 env_outputs.weight))\n",
    "            entropy_loss = -hparams.entropy_scale * tf.reduce_sum(\n",
    "                 agent_estimates_output.entropy * env_outputs.weight)\n",
    "            loss = (policy_loss + value_loss + entropy_loss) / (explore_size * max_steps)\n",
    "\n",
    "        variables = agent_model.trainable_variables\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based On-Policy PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardOutput = collections.namedtuple(\n",
    "    'ForwardOutput', ['deltas_norm'])\n",
    "\n",
    "\n",
    "class CartPoleEnv(tf.Module):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(CartPoleEnv, self).__init__(name='CartPoleEnv')\n",
    "        self._hidden = tf.keras.layers.Dense(32, activation=tf.nn.relu)\n",
    "        self._logits = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(8, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(4)])\n",
    "        self.deltas_moments = pynr.moments.StreamingMoments([4])\n",
    "        self.output_specs = env.output_specs\n",
    "        self.output_shapes = tf.nest.map_structure(\n",
    "            lambda spec: spec.shape, self.output_specs)\n",
    "        self.output_dtypes = tf.nest.map_structure(\n",
    "            lambda spec: spec.dtype, self.output_specs)\n",
    "\n",
    "    @tf.function\n",
    "    def _scale_state(self, state):\n",
    "        state = (state / [[2.4, 10., 1., 10.]])\n",
    "        state = tf.concat(\n",
    "            [state, tf.stack([tf.math.cos(state[..., 2] / math.pi),\n",
    "                              tf.math.sin(state[..., 2] / math.pi)],\n",
    "                             axis=-1)],\n",
    "            axis=-1)\n",
    "        return tf.clip_by_value(state, -1., 1.)\n",
    "\n",
    "    @tf.function\n",
    "    def terminals(self, next_state, time_step):\n",
    "        is_terminal = pynr.debugging.mock_spec(\n",
    "            next_state.shape[:1], \n",
    "            self.output_specs.terminal, \n",
    "            tf.ones)\n",
    "\n",
    "        if time_step > 200:\n",
    "            return is_terminal\n",
    "\n",
    "        state_abs = tf.abs(next_state)\n",
    "        return tf.where(\n",
    "            tf.logical_or(tf.greater(state_abs[:, 0], 2.4), \n",
    "                          tf.greater(state_abs[:, 2], 12.)), \n",
    "            is_terminal, ~is_terminal)\n",
    "\n",
    "    @tf.function\n",
    "    def rewards(self, next_state):\n",
    "        return pynr.debugging.mock_spec(\n",
    "            next_state.shape[:1], \n",
    "            self.output_specs.reward, \n",
    "            tf.ones)\n",
    "\n",
    "    @tf.function\n",
    "    def forward(self, env_outputs, agent_outputs):\n",
    "        state = self._scale_state(env_outputs.state)\n",
    "        hidden = self._hidden(\n",
    "            tf.concat([\n",
    "                state, \n",
    "                tf.cast(agent_outputs.action[..., None], \n",
    "                        tf.dtypes.float32)\n",
    "            ], axis=-1))\n",
    "        deltas_norm = self._logits(hidden)\n",
    "        return ForwardOutput(deltas_norm=deltas_norm)\n",
    "\n",
    "    @tf.function\n",
    "    def reset(self, size, seed):\n",
    "        seed = tfp.distributions.SeedStream(seed, salt='forward_reset')\n",
    "        states_loc = [0., 0., 0., 0.]\n",
    "        states_scale_diag = [.25, .25, .25, .25]\n",
    "        initial_state_distribution = tfp.distributions.MultivariateNormalDiag(\n",
    "            loc=states_loc, scale_diag=states_scale_diag)\n",
    "        next_initial_state = initial_state_distribution.sample([size], seed=seed())\n",
    "        initial_state = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([size]), self.output_specs.state)\n",
    "        initial_reward = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([size]), self.output_specs.reward)\n",
    "        initial_terminal = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([size]), self.output_specs.terminal)\n",
    "        initial_weight = pynr.debugging.mock_spec(\n",
    "            tf.TensorShape([size]), self.output_specs.weight, \n",
    "            tf.ones)\n",
    "        return pyrl.rollouts.Transition(\n",
    "            state=initial_state, \n",
    "            next_state=next_initial_state,\n",
    "            reward=initial_reward,\n",
    "            terminal=initial_terminal,\n",
    "            weight=initial_weight)\n",
    "\n",
    "    @tf.function\n",
    "    def step(self, agent_outputs, env_outputs, time_step):\n",
    "        state = env_outputs.next_state\n",
    "        state = self._scale_state(state)\n",
    "        action = agent_outputs.action\n",
    "        hidden = self._hidden(\n",
    "            tf.concat([\n",
    "                state, \n",
    "                tf.cast(agent_outputs.action[..., None], \n",
    "                        tf.dtypes.float32)\n",
    "            ], axis=-1))\n",
    "        deltas_norm = self._logits(hidden)\n",
    "        deltas = self.deltas_moments.denormalize(\n",
    "            deltas_norm, env_outputs.weight[..., None])\n",
    "\n",
    "        delta_high = self.deltas_moments.mean + 3. * self.deltas_moments.std\n",
    "        deltas = tf.clip_by_value(deltas, -delta_high, delta_high)\n",
    "\n",
    "        next_state = env_outputs.next_state + deltas\n",
    "        terminal = self.terminals(next_state, time_step)\n",
    "        terminal = tf.logical_or(terminal, env_outputs.terminal)\n",
    "        reward = self.rewards(next_state)\n",
    "        weight = tf.cast(~env_outputs.terminal, tf.dtypes.float32)\n",
    "\n",
    "        return pyrl.rollouts.Transition(\n",
    "            state=env_outputs.next_state, \n",
    "            next_state=next_state,\n",
    "            reward=reward,\n",
    "            terminal=terminal,\n",
    "            weight=weight)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForwardModel(object):\n",
    "\n",
    "    def __init__(self, env, size):\n",
    "        self.env = env\n",
    "        self.size = size\n",
    "        self._seed = tf.Variable(0, trainable=False)\n",
    "\n",
    "    def seed(self, random_seed):\n",
    "        self._seed.assign(random_seed)\n",
    "\n",
    "    @tf.function\n",
    "    def reset(self, *args, **kwargs):\n",
    "        return self.env.reset(\n",
    "            *args, \n",
    "            size=self.size, \n",
    "            seed=self._seed,\n",
    "            **kwargs)\n",
    "\n",
    "    @tf.function\n",
    "    def step(self, *args, **kwargs):\n",
    "        return self.env.step(*args, **kwargs)\n",
    "\n",
    "ForwardHyperParameters = collections.namedtuple(\n",
    "    'ForwardHyperParameters', \n",
    "    ['iterations',\n",
    "     'epochs',\n",
    "     'learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_size = 32\n",
    "exploit_size = 128\n",
    "forward_explore_size = 512\n",
    "forward_exploit_size = 128\n",
    "max_steps = 500\n",
    "\n",
    "explore_env_model = create_env_model(explore_size)\n",
    "exploit_env_model = create_env_model(exploit_size)\n",
    "\n",
    "forward_model = CartPoleEnv(explore_env_model)\n",
    "agent_model = CartPoleAgent(explore_env_model.action_spec)\n",
    "\n",
    "explore_strategy = Strategy(agent_model, True)\n",
    "exploit_strategy = Strategy(agent_model, False)\n",
    "\n",
    "explore_rollout = pyrl.rollouts.Rollout(explore_env_model, explore_strategy, max_steps)\n",
    "exploit_rollout = pyrl.rollouts.Rollout(exploit_env_model, exploit_strategy, max_steps)\n",
    "\n",
    "explore_forward_model = ForwardModel(forward_model, forward_explore_size)\n",
    "exploit_forward_model = ForwardModel(forward_model, forward_exploit_size)\n",
    "\n",
    "explore_forward_rollout = pyrl.rollouts.Rollout(explore_forward_model, explore_strategy, max_steps)\n",
    "exploit_forward_rollout = pyrl.rollouts.Rollout(exploit_forward_model, exploit_strategy, max_steps)\n",
    "\n",
    "hparams = HyperParameters(\n",
    "    # iterations=10,\n",
    "    iterations=1,\n",
    "    epochs=10,\n",
    "    discounts=.99,\n",
    "    lambdas=.975,\n",
    "    epsilon=.2,\n",
    "    value_scale=.5,\n",
    "    entropy_scale=.05,\n",
    "    eval_every=10,\n",
    "    learning_rate=1e-3,\n",
    ")\n",
    "forward_hparams = ForwardHyperParameters(\n",
    "    # iterations=30,\n",
    "    iterations=1,\n",
    "    epochs=15,\n",
    "    learning_rate=1e-2,\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(hparams.learning_rate)\n",
    "forward_optimizer = tf.keras.optimizers.Adam(forward_hparams.learning_rate)\n",
    "\n",
    "discounted_returns = tf.function(pyrl.targets.discounted_returns)\n",
    "generalized_advantage_estimate = tf.function(pyrl.targets.generalized_advantage_estimate)\n",
    "\n",
    "explore_env_model.seed(42)\n",
    "explore_forward_model.seed(42)\n",
    "for forward_iteration in range(forward_hparams.iterations):    \n",
    "    (agent_outputs, env_outputs) = explore_rollout().outputs\n",
    "\n",
    "    # Train the forward model\n",
    "    deltas = (env_outputs.next_state - env_outputs.state)\n",
    "    forward_model.deltas_moments.update_state(deltas, env_outputs.weight[..., None])\n",
    "    deltas_norm = forward_model.deltas_moments.normalize(\n",
    "        deltas, env_outputs.weight[..., None])\n",
    "\n",
    "    for _ in range(forward_hparams.epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            forward_estimates_output = forward_model.forward(\n",
    "                env_outputs, agent_outputs)\n",
    "            forward_loss = tf.reduce_mean(\n",
    "                tf.square(deltas_norm - forward_estimates_output.deltas_norm), \n",
    "                axis=-1)\n",
    "            loss = tf.reduce_sum(\n",
    "                forward_loss * env_outputs.weight)\n",
    "            loss = loss / (explore_size * max_steps)\n",
    "\n",
    "        variables = forward_model.trainable_variables\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        forward_optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "    # Train the policy\n",
    "    for iteration in range(hparams.iterations):\n",
    "        if (iteration % hparams.eval_every) == 0:\n",
    "            exploit_forward_model.seed(42 + forward_explore_size + 1)\n",
    "            (_, eval_forward_outputs) = exploit_forward_rollout().outputs\n",
    "            returns = tf.reduce_sum(eval_forward_outputs.reward * eval_forward_outputs.weight, axis=1)\n",
    "            tf.print('Forward', tf.reduce_mean(returns))\n",
    "\n",
    "            exploit_env_model.seed(42 + explore_size + 1)\n",
    "            (_, eval_env_outputs) = exploit_rollout().outputs\n",
    "            returns = tf.reduce_sum(eval_env_outputs.reward * eval_env_outputs.weight, axis=1)\n",
    "            tf.print('Real World', tf.reduce_mean(returns))\n",
    "\n",
    "        (agent_outputs, env_outputs) = explore_forward_rollout().outputs\n",
    "        agent_value_outputs = agent_model.value(env_outputs, agent_outputs)\n",
    "        returns = discounted_returns(\n",
    "            env_outputs.reward * env_outputs.weight, discounts=hparams.discounts)\n",
    "        advantages = generalized_advantage_estimate(\n",
    "            env_outputs.reward * env_outputs.weight, agent_value_outputs.value * env_outputs.weight,\n",
    "            discounts=hparams.discounts, lambdas=hparams.lambdas, weights=env_outputs.weight)\n",
    "\n",
    "        for _ in range(hparams.epochs):\n",
    "            with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "                tape.watch(agent_model.trainable_variables)\n",
    "\n",
    "                agent_estimates_output = agent_model.policy_value(\n",
    "                    env_outputs, agent_outputs)\n",
    "\n",
    "                ratio = tf.exp(\n",
    "                    agent_estimates_output.log_prob - agent_outputs.log_prob)\n",
    "                surrogate1 = ratio * advantages\n",
    "                surrogate2 = tf.clip_by_value(\n",
    "                    ratio,\n",
    "                    1 - hparams.epsilon,\n",
    "                    1 + hparams.epsilon) * advantages\n",
    "                surrogate_loss = tf.minimum(surrogate1, surrogate2)\n",
    "                policy_loss = -tf.reduce_sum(\n",
    "                    surrogate_loss * env_outputs.weight)\n",
    "                value_loss = hparams.value_scale * tf.reduce_sum(\n",
    "                    (tf.square(agent_estimates_output.value - returns) *\n",
    "                     env_outputs.weight))\n",
    "                entropy_loss = -hparams.entropy_scale * tf.reduce_sum(\n",
    "                     agent_estimates_output.entropy * env_outputs.weight)\n",
    "                loss = (policy_loss + value_loss + entropy_loss) / (explore_size * max_steps)\n",
    "\n",
    "            variables = agent_model.trainable_variables\n",
    "            grads = tape.gradient(loss, variables)\n",
    "            optimizer.apply_gradients(zip(grads, variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On/Off-Policy IMPALA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def actor(host, port, actor_id, num_actors):\n",
    "    print('Starting Actor!')\n",
    "    max_steps = 500\n",
    "    explore_env_model = create_env_model()\n",
    "    agent_model = CartPoleAgent(explore_env_model.action_spec)\n",
    "    explore_strategy = Strategy(agent_model, True)\n",
    "    explore_rollout = pyrl.rollouts.Rollout(explore_env_model, explore_strategy, max_steps)\n",
    "\n",
    "    mock_env_outputs = pynr.debugging.mock_spec(\n",
    "        tf.TensorShape([1, max_steps]), explore_env_model.output_specs)\n",
    "    mock_agent_outputs = pynr.debugging.mock_spec(\n",
    "        tf.TensorShape([1, max_steps]), agent_model.output_specs)\n",
    "    agent_model.initialize(mock_env_outputs, mock_agent_outputs)\n",
    "\n",
    "    # Connect to the redis server.\n",
    "    pipe = redis.Redis(host=host, port=port, db=0)\n",
    "\n",
    "    # Control flow for rollouts.\n",
    "    cond = pynr.distributed.Condition(\n",
    "        pipe, 'WaitCondition')\n",
    "\n",
    "    # Queue for rollouts.\n",
    "    queue = pynr.distributed.Queue(\n",
    "        pipe, 'RolloutQueue',\n",
    "        dtypes=(tf.dtypes.int32, \n",
    "                (agent_model.output_dtypes, explore_env_model.output_dtypes)))\n",
    "\n",
    "    # Parameter server.\n",
    "    parameters = pynr.distributed.Register(\n",
    "        pipe, 'Parameters',\n",
    "        dtypes=tf.nest.map_structure(lambda var: var.dtype,\n",
    "                                     agent_model.variables))\n",
    "\n",
    "    # Queried to determine when to sync parameters.\n",
    "    sync = pynr.distributed.MultiEvent(\n",
    "        pipe, actor_id, num_actors, 'SyncParameters')\n",
    "\n",
    "    explore_env_model.seed(42 + actor_id)\n",
    "    while True:\n",
    "        cond.wait(actor_id)\n",
    "        # Sync parameters only if we need to.\n",
    "        if sync.get():\n",
    "            sync.unset()\n",
    "            tf.nest.map_structure(\n",
    "                lambda dst, src: dst.assign(src),\n",
    "                agent_model.variables,\n",
    "                parameters.get())\n",
    "        values = explore_rollout().outputs\n",
    "        queue.enqueue((tf.cast(actor_id, tf.dtypes.int32), \n",
    "                       values))\n",
    "\n",
    "\n",
    "def learner(host, port, num_actors):\n",
    "    batch_size = 32\n",
    "    exploit_size = 16\n",
    "    max_steps = 500\n",
    "    exploit_env_model = create_env_model()\n",
    "    agent_model = CartPoleAgent(exploit_env_model.action_spec)\n",
    "    exploit_strategy = Strategy(agent_model, False)\n",
    "    exploit_rollout = pyrl.rollouts.Rollout(exploit_env_model, exploit_strategy, max_steps)\n",
    "\n",
    "    mock_env_outputs = pynr.debugging.mock_spec(\n",
    "        tf.TensorShape([1, max_steps]), exploit_env_model.output_specs)\n",
    "    mock_agent_outputs = pynr.debugging.mock_spec(\n",
    "        tf.TensorShape([1, max_steps]), agent_model.output_specs)\n",
    "    agent_model.initialize(mock_env_outputs, mock_agent_outputs)\n",
    "\n",
    "    hparams = HyperParameters(\n",
    "        iterations=100,\n",
    "        discounts=.99,\n",
    "        value_scale=.5,\n",
    "        epochs=None,\n",
    "        lambdas=None,\n",
    "        epsilon=None,\n",
    "        entropy_scale=.05,\n",
    "        eval_every=1,\n",
    "        learning_rate=1e-3,\n",
    "    )\n",
    "    optimizer = tf.keras.optimizers.Adam(hparams.learning_rate)\n",
    "    v_trace_returns = tf.function(pyrl.targets.v_trace_returns)\n",
    "    temporal_difference = tf.function(pyrl.targets.temporal_difference)\n",
    "\n",
    "    # Connect to the redis server.\n",
    "    pipe = redis.Redis(host=host, port=port, db=0)\n",
    "\n",
    "    # Control flow for rollouts.\n",
    "    cond = pynr.distributed.Condition(\n",
    "        pipe, 'WaitCondition')\n",
    "\n",
    "    # Queue for rollouts.\n",
    "    queue = pynr.distributed.Queue(\n",
    "        pipe, 'RolloutQueue',\n",
    "        dtypes=(tf.dtypes.int32, \n",
    "                (agent_model.output_dtypes, exploit_env_model.output_dtypes)))\n",
    "\n",
    "    # Parameter server.\n",
    "    parameters = pynr.distributed.Register(\n",
    "        pipe, 'Parameters',\n",
    "        dtypes=tf.nest.map_structure(lambda var: var.dtype,\n",
    "                                     agent_model.variables))\n",
    "\n",
    "    # Queried to determine when to sync parameters.\n",
    "    sync = pynr.distributed.MultiEvent(\n",
    "        pipe, num_actors, num_actors, 'SyncParameters')\n",
    "\n",
    "    def reader_fn():\n",
    "        while True:\n",
    "            actor_id, values = queue.dequeue()\n",
    "            cond.notify(actor_id)\n",
    "            yield tf.nest.map_structure(\n",
    "                lambda t: tf.squeeze(t, axis=0), values)\n",
    "\n",
    "    def set_n_step_shape_fn(shape):\n",
    "        return tf.TensorShape([max_steps] + shape.as_list())\n",
    "\n",
    "    agent_output_shapes = tf.nest.map_structure(\n",
    "        set_n_step_shape_fn, agent_model.output_shapes)\n",
    "    env_output_shapes = tf.nest.map_structure(\n",
    "        set_n_step_shape_fn, exploit_env_model.output_shapes)\n",
    "\n",
    "    # Stage N batches ahead of time\n",
    "    prefetch_size = 1\n",
    "    actor = tf.data.Dataset.from_generator(\n",
    "        reader_fn,\n",
    "        output_types=(agent_model.output_dtypes, exploit_env_model.output_dtypes),\n",
    "        output_shapes=(agent_output_shapes, env_output_shapes))\n",
    "    actor = actor.batch(batch_size)\n",
    "    actor = actor.prefetch(prefetch_size)\n",
    "    actor_reader = iter(actor)\n",
    "    \n",
    "    print('Starting learner!')\n",
    "    for iteration in range(hparams.iterations):\n",
    "        parameters.set(agent_model.variables)\n",
    "        sync.set_all()\n",
    "        cond.notify_all()\n",
    "\n",
    "        if (iteration % hparams.eval_every) == 0:\n",
    "            exploit_env_model.seed(42 + num_actors + 1)\n",
    "            (_, eval_env_outputs) = exploit_rollout().outputs\n",
    "            returns = tf.reduce_sum(eval_env_outputs.reward * eval_env_outputs.weight, axis=1)\n",
    "            tf.print(tf.reduce_mean(returns))\n",
    "\n",
    "        (agent_outputs, env_outputs) = next(actor_reader)\n",
    "\n",
    "        # Estimate gradients here.\n",
    "        with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "            tape.watch(agent_model.trainable_variables)\n",
    "\n",
    "            (agent_estimates_output,\n",
    "             agent_value_output_last) = agent_model.policy_value_with_nexts(\n",
    "                 env_outputs, agent_outputs)\n",
    "\n",
    "            masked_discounts = tf.cast(\n",
    "                ~env_outputs.terminal, tf.dtypes.float32) * hparams.discounts\n",
    "            returns = v_trace_returns(\n",
    "                env_outputs.reward * env_outputs.weight,\n",
    "                agent_estimates_output.value * env_outputs.weight,\n",
    "                agent_estimates_output.log_prob * env_outputs.weight,\n",
    "                agent_outputs.log_prob * env_outputs.weight,\n",
    "                last_value=agent_value_output_last.value,\n",
    "                discounts=masked_discounts,\n",
    "                weights=env_outputs.weight)\n",
    "\n",
    "            returns_next = tf.concat(\n",
    "                [returns[:, 1:], tf.expand_dims(\n",
    "                    agent_value_output_last.value, axis=1)],\n",
    "                axis=1)\n",
    "\n",
    "            clipped_is = tf.math.minimum(\n",
    "                1., tf.exp(\n",
    "                    agent_estimates_output.log_prob - agent_outputs.log_prob))\n",
    "            clipped_is = tf.stop_gradient(clipped_is)\n",
    "            returns = tf.stop_gradient(\n",
    "                env_outputs.reward + masked_discounts * returns_next)\n",
    "\n",
    "            advantages = clipped_is * temporal_difference(\n",
    "                returns * env_outputs.weight,\n",
    "                agent_estimates_output.value * env_outputs.weight,\n",
    "                back_prop=True)\n",
    "\n",
    "            policy_loss = -tf.reduce_sum(\n",
    "                (agent_estimates_output.log_prob * advantages) *\n",
    "                env_outputs.weight)\n",
    "            value_loss = hparams.value_scale * tf.reduce_sum(\n",
    "                (tf.square(advantages) * env_outputs.weight))\n",
    "            entropy_loss = -hparams.entropy_scale * tf.reduce_sum(\n",
    "                agent_estimates_output.entropy * env_outputs.weight)\n",
    "            loss = ((policy_loss + value_loss + entropy_loss) /\n",
    "                    (batch_size * max_steps))\n",
    "\n",
    "        variables = agent_model.trainable_variables\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Actor!\n",
      "Starting Actor!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 11:29:35.740985 123145393065984 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0903 11:29:35.741000 123145511772160 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 11:29:37.700404 140736428594112 deprecation.py:323] From /Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/data/ops/dataset_ops.py:505: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, there are two\n",
      "    options available in V2.\n",
      "    - tf.py_function takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    - tf.numpy_function maintains the semantics of the deprecated tf.py_func\n",
      "    (it is not differentiable, and manipulates numpy arrays). It drops the\n",
      "    stateful argument making all functions stateful.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting learner!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0903 11:29:37.779468 140736428594112 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.string\n",
      "W0903 11:29:38.220720 123145442127872 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "W0903 11:29:39.292039 123145511235584 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 11:29:39.295121 123145511235584 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 11:29:39.298089 123145511235584 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 11:29:39.301657 123145511235584 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 11:29:42.295219 123145393602560 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 11:29:42.298702 123145393602560 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 11:29:42.301769 123145393602560 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 11:29:42.305060 123145394675712 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 11:29:42.523055 123145441054720 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 11:29:42.525872 123145440518144 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n",
      "W0903 11:29:42.528542 123145440518144 backprop.py:842] The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0903 11:29:44.066809 140736428594112 deprecation.py:323] From /Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "19\n",
      "11\n",
      "10\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "18\n",
      "26\n",
      "45\n",
      "74\n",
      "119\n",
      "500\n",
      "500\n",
      "78\n",
      "63\n",
      "55\n",
      "51\n",
      "47\n",
      "46\n",
      "46\n",
      "45\n",
      "47\n",
      "48\n",
      "51\n",
      "56\n",
      "64\n",
      "89\n",
      "500\n",
      "102\n",
      "69\n",
      "57\n",
      "53\n",
      "46\n",
      "39\n",
      "35\n",
      "27\n",
      "22\n",
      "20\n",
      "17\n",
      "17\n",
      "20\n",
      "25\n",
      "34\n",
      "53\n",
      "69\n",
      "106\n",
      "208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-3:\n",
      "Traceback (most recent call last):\n",
      "Process Process-1:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-11-636d27c59608>\", line 135, in learner\n",
      "    cond.notify_all()\n",
      "  File \"/Users/samwenke/code/pyoneer/pyoneer/distributed/distributed_ops.py\", line 227, in notify_all\n",
      "    tf.py_function(notify_all_fn, (), ())\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"<ipython-input-11-636d27c59608>\", line 50, in actor\n",
      "    values))\n",
      "  File \"/Users/samwenke/code/pyoneer/pyoneer/distributed/distributed_ops.py\", line 120, in enqueue\n",
      "    buffer = self.encode(structure)\n",
      "  File \"/Users/samwenke/code/pyoneer/pyoneer/distributed/distributed_ops.py\", line 60, in encode\n",
      "    _ = tf.nest.map_structure(encode_w_lengths, item)\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 515, in map_structure\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 515, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"/Users/samwenke/code/pyoneer/pyoneer/distributed/distributed_ops.py\", line 57, in encode_w_lengths\n",
      "    lengths.append(tf.cast(tf.strings.length(msg), self._length_dtype))\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/string_ops.py\", line 375, in string_length_v2\n",
      "    return string_length(input, name, unit)\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\", line 180, in wrapper\n",
      "    return target(*args, **kwargs)\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/string_ops.py\", line 369, in string_length\n",
      "    return gen_string_ops.string_length(input, unit=unit, name=name)\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/gen_string_ops.py\", line 1007, in string_length\n",
      "    unit)\n",
      "KeyboardInterrupt\n",
      "Process Process-2:\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 392, in eager_py_func\n",
      "    return _internal_py_func(func=func, inp=inp, Tout=Tout, eager=True, name=name)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 281, in _internal_py_func\n",
      "    input=inp, token=token, Tout=Tout, name=name)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 65, in eager_py_func\n",
      "    _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/samwenke/code/pyoneer/pyoneer/distributed/distributed_ops.py\", line 271, in get\n",
      "    tf.nest.flatten([self.dtypes]))\n",
      "  File \"<ipython-input-11-636d27c59608>\", line 47, in actor\n",
      "    parameters.get())\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 392, in eager_py_func\n",
      "    return _internal_py_func(func=func, inp=inp, Tout=Tout, eager=True, name=name)\n",
      "tensorflow.python.framework.errors_impl.UnknownError: KeyboardInterrupt: \n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\n",
      "    return func(device, token, args)\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 109, in __call__\n",
      "    ret = self._func(*args)\n",
      "\n",
      "  File \"/Users/samwenke/code/pyoneer/pyoneer/distributed/distributed_ops.py\", line 219, in notify_all_fn\n",
      "    w_id = self._pipe.lpop(self._key)\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/redis/client.py\", line 1675, in lpop\n",
      "    return self.execute_command('LPOP', name)\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/redis/client.py\", line 839, in execute_command\n",
      "    return self.parse_response(conn, command_name, **options)\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/redis/client.py\", line 853, in parse_response\n",
      "    response = connection.read_response()\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/redis/connection.py\", line 699, in read_response\n",
      "    response = self._parser.read_response()\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/redis/connection.py\", line 313, in read_response\n",
      "    byte, response = byte_to_chr(response[0]), response[1:]\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/redis/_compat.py\", line 119, in byte_to_chr\n",
      "    return chr(x)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      " [Op:EagerPyFunc]\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 281, in _internal_py_func\n",
      "    input=inp, token=token, Tout=Tout, name=name)\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py\", line 65, in eager_py_func\n",
      "    _six.raise_from(_core._status_to_exception(e.code, message), None)\n",
      "  File \"<string>\", line 3, in raise_from\n",
      "tensorflow.python.framework.errors_impl.UnknownError: KeyboardInterrupt: \n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 207, in __call__\n",
      "    return func(device, token, args)\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 109, in __call__\n",
      "    ret = self._func(*args)\n",
      "\n",
      "  File \"/Users/samwenke/code/pyoneer/pyoneer/distributed/distributed_ops.py\", line 268, in get_fn\n",
      "    return tf.nest.flatten([self.decode(item)])\n",
      "\n",
      "  File \"/Users/samwenke/code/pyoneer/pyoneer/distributed/distributed_ops.py\", line 89, in decode\n",
      "    self._dtypes, tf.unstack(lengths, axis=0))\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 450, in pack_sequence_as\n",
      "    return _sequence_like(structure, packed)\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 134, in _sequence_like\n",
      "    elif _is_namedtuple(instance) or _is_attrs(instance):\n",
      "\n",
      "  File \"/Users/samwenke/.local/share/virtualenvs/pyoneer-K_ZVJbe4/lib/python3.6/site-packages/tensorflow/python/util/nest.py\", line 106, in _is_namedtuple\n",
      "    return _pywrap_tensorflow.IsNamedtuple(instance, strict)\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n",
      " [Op:EagerPyFunc]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-fd91dc8689c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mlearner_proc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_actors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mlearner_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mlearner_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mactor_processes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_pid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a child process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'can only join a started process'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0m_children\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     48\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# This shouldn't block if wait() returned successfully.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWNOHANG\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, flag)\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                     \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwaitpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                     \u001b[0;31m# Child process not yet created. See #1731717\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_actors = 2\n",
    "host = '127.0.0.1'\n",
    "port = '6380'\n",
    "\n",
    "actor_processes = []\n",
    "for actor_id in range(num_actors):\n",
    "    p = multiprocessing.Process(target=actor, args=(host, port, actor_id, num_actors,))\n",
    "    p.start()\n",
    "    actor_processes.append(p)\n",
    "\n",
    "time.sleep(2)\n",
    "learner_proc = multiprocessing.Process(target=learner, args=(host, port, num_actors,))\n",
    "learner_proc.start()\n",
    "learner_proc.join()\n",
    "for p in actor_processes:\n",
    "    p.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyoneer-K_ZVJbe4",
   "language": "python",
   "name": "pyoneer-k_zvjbe4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
